{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecc49cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import cat\n",
    "from torchvision import transforms\n",
    "from sklearn.utils import gen_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af696afd",
   "metadata": {},
   "source": [
    "# Data Importation and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81dc59bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37779 observations and each observation has 7 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Importation of the data\n",
    "filename = './data/meta_data.hdf'\n",
    "row_data = pd.read_hdf(filename,'/d')\n",
    "\n",
    "print(f'There are {row_data.shape[0]} observations and each observation has {row_data.shape[1]} attributes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3486505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1620 validated observations and each observation has 7 attributes.\n"
     ]
    }
   ],
   "source": [
    "# Here we merge all data (csv) together from the validation part \n",
    "all_files = ['paths_df_validated_3000_5500.csv', 'paths_df_validated_5500_6500.csv']\n",
    "final_data = pd.DataFrame()\n",
    "\n",
    "for file in all_files : \n",
    "    temp = pd.read_csv(file)\n",
    "    temp = temp.loc[temp['valid'] == True] #Select only the 'True' = Validated images\n",
    "    final_data = pd.concat([final_data,temp],axis=0) #Concatenate the information in one dataframe\n",
    "\n",
    "# We select the images from row_data with the index of the validation part (final_data)\n",
    "index_name = final_data['original_index'].values    # Select the index\n",
    "data = row_data.iloc[index_name]                    # Extract the validated images\n",
    "\n",
    "print(f'There are {data.shape[0]} validated observations and each observation has {data.shape[1]} attributes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f812eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All is good with the dataset, we can continue !\n"
     ]
    }
   ],
   "source": [
    "# This bloc checks if the selected images are the good ones\n",
    "n_match = (final_data['mask_path'].values == data['building_id'].values).sum()\n",
    "if n_match == data.shape[0] :\n",
    "    print('All is good with the dataset, we can continue !')\n",
    "else : \n",
    "    print('There is a mismatch between pictures (pictures names)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcdd78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bloc plots the picture and the corresponding polygon surfaces.\n",
    "showing = False \n",
    "\n",
    "# This function plots the surface polygons of the selected image \n",
    "def image_reader (image_data) : \n",
    "    surfaces = image_data.b_surfaces\n",
    "    # One polygone\n",
    "    if (len(surfaces[0])==2) : \n",
    "        plt.plot(np.array(surfaces)[:,1],np.array(surfaces)[:,0], label = 'surface', c = 'r')\n",
    "    # Multiples polygones\n",
    "    else :\n",
    "        for polygons in surfaces : \n",
    "            plt.plot(np.array(polygons)[:,1],np.array(polygons)[:,0], label = 'surface', c = 'r')\n",
    "    return 0\n",
    "\n",
    "# The for-loop that will plot the pictures \n",
    "if showing :\n",
    "    n = 1\n",
    "    for i in range (n) : \n",
    "        a = data.iloc[i]\n",
    "        im_name = a.building_id +  '-b15-otovowms.jpeg'\n",
    "        print(im_name)\n",
    "        im_matrix = plt.imread('./data/' + im_name)\n",
    "        plt.imshow(im_matrix)\n",
    "        image_reader(a)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ead431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates the mask for one image, the mask is a array (500*500)\n",
    "def mask_creation (n,p,surfaces) : \n",
    "    # If one polygon in surface list\n",
    "    if (len(surfaces[0])==2) : \n",
    "        x, y = np.meshgrid(np.arange(n), np.arange(p))\n",
    "        x, y = x.flatten(), y.flatten()\n",
    "        points = np.vstack((x,y)).T\n",
    "        path = Path(surfaces)\n",
    "        grid = path.contains_points(points)\n",
    "        grid = grid.reshape((n,p))\n",
    "    # If multiple polygones in surface list\n",
    "    else :\n",
    "        grid = np.reshape([False for i in range (n*p)],(n,p))\n",
    "        for i,polygons in enumerate(surfaces) : \n",
    "            x, y = np.meshgrid(np.arange(n), np.arange(p))\n",
    "            x, y = x.flatten(), y.flatten()\n",
    "            points = np.vstack((x,y)).T\n",
    "            path = Path(polygons)\n",
    "            grid_2 = path.contains_points(points)\n",
    "            grid_2 = grid_2.reshape((n,p))\n",
    "            grid = np.add(grid,grid_2)\n",
    "    # We return the 500*500 matrix filled with True and False\n",
    "    return (grid)\n",
    "\n",
    "\n",
    "\n",
    "# This function creates a format that will be use for the DL Network\n",
    "def creation_useful_dataset(dataset) : \n",
    "    # Creation of variables \n",
    "    X=[]; Y=[];\n",
    "    length,_ = dataset.shape\n",
    "    n = 500; p = 500; # Image Shape\n",
    "    for i in tqdm(range(length)) :\n",
    "        # Import the wanted picture and the matrix\n",
    "        info = dataset.iloc[i]\n",
    "        image_name = info.building_id + '-b15-otovowms.jpeg'\n",
    "        im_matrix = plt.imread('./data/' + image_name)\n",
    "        # We build the X_dataset \n",
    "        X.append(im_matrix)\n",
    "        # We build the target_dataset\n",
    "        surfaces = info.b_surfaces\n",
    "        Y.append(mask_creation(n,p,surfaces))\n",
    "    # We convert the array in pytorch Tensor \n",
    "    X = torch.Tensor(np.array(X))\n",
    "    X = torch.swapaxes(X, 1, -1)  # To have (B,C,H,W)\n",
    "    Y = torch.Tensor(np.array(Y)) \n",
    "    return (X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f8e875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importation of the Training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1296/1296 [02:16<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importation of the Testing set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 324/324 [00:33<00:00,  9.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# We create a split 70% Training set, 15% validation set, 15% testing set \n",
    "n_split1 = int(data.shape[0]*0.70) \n",
    "n_split2 = int(data.shape[0]*0.85)\n",
    "df_train = data.iloc[:n_split1]\n",
    "df_validation = data.iloc[n_split1:n_split2]\n",
    "df_test = data.iloc[n_split2:]\n",
    "\n",
    "print('Importation of the Training set')\n",
    "X_train, Y_train = creation_useful_dataset(df_train)\n",
    "print('Importation of the Validation set')\n",
    "X_validation, Y_validation = creation_useful_dataset(df_train)\n",
    "print('Importation of the Testing set')\n",
    "X_test, Y_test = creation_useful_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the data into a pickle file\n",
    "with open('./pickles/data.X_train', 'wb') as f:\n",
    "     pickle.dump(X_train, f)\n",
    "with open('./pickles/data.Y_train', 'wb') as f:\n",
    "     pickle.dump(Y_train, f)\n",
    "        \n",
    "with open('./pickles/data.X_validation', 'wb') as f:\n",
    "     pickle.dump(X_validation, f)\n",
    "with open('./pickles/data.Y_validation', 'wb') as f:\n",
    "     pickle.dump(Y_validation, f)\n",
    "with open('./pickles/data.X_test', 'wb') as f:\n",
    "     pickle.dump(X_test, f)\n",
    "with open('./pickles/data.Y_test', 'wb') as f:\n",
    "     pickle.dump(Y_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa499a8",
   "metadata": {},
   "source": [
    "**Data Augmentation** \n",
    "\n",
    "We can shift the image 90°,180°,270° to have more images to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45ef6b5",
   "metadata": {},
   "source": [
    "# Deep Learning Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d43cbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from a pickle file\n",
    "with open('./pickles/data.X_train', 'rb') as f:\n",
    "     X_train = pickle.load(f)\n",
    "with open('./pickles/data.Y_train', 'rb') as f:\n",
    "     Y_train = pickle.load(f)\n",
    "with open('./pickles/data.X_validation', 'rb') as f:\n",
    "     X_validation = pickle.load(f)\n",
    "with open('./pickles/data.Y_validation', 'rb') as f:\n",
    "     Y_validation = pickle.load(f)\n",
    "with open('./pickles/data.X_test', 'rb') as f:\n",
    "     X_test = pickle.load(f)\n",
    "with open('./pickles/data.Y_test', 'rb') as f:\n",
    "     Y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0651954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : torch.Size([1296, 3, 500, 500]) - [Batchsize,Channel,Height,Width]\n",
      "Y_train : torch.Size([1296, 500, 500]) - [Batchsize,Height,Width]\n",
      "X_validation : torch.Size([75, 3, 500, 500]) - [Batchsize,Channel,Height,Width]\n",
      "Y_validation : torch.Size([75, 500, 500]) - [Batchsize,Height,Width]\n",
      "X_test : torch.Size([24, 3, 500, 500]) - [Batchsize,Channel,Height,Width]\n",
      "Y_test : torch.Size([24, 500, 500]) - [Batchsize,Height,Width]\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train : {X_train.shape} - [Batchsize,Channel,Height,Width]')\n",
    "print(f'Y_train : {Y_train.shape} - [Batchsize,Height,Width]')\n",
    "print(f'X_validation : {X_validation.shape} - [Batchsize,Channel,Height,Width]')\n",
    "print(f'Y_validation : {Y_validation.shape} - [Batchsize,Height,Width]')\n",
    "print(f'X_test : {X_test.shape} - [Batchsize,Channel,Height,Width]')\n",
    "print(f'Y_test : {Y_test.shape} - [Batchsize,Height,Width]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d332fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dataset_for_test = False \n",
    "\n",
    "## For testing the network, we take a smaller dataset \n",
    "if small_dataset_for_test : \n",
    "    n_split = 3\n",
    "    df_train = data.iloc[:n_split]\n",
    "    df_validation = data.iloc[n_split:n_split+2]\n",
    "    df_test = data.iloc[n_split+2:n_split+4]\n",
    "    print('Importation of the Training set')\n",
    "    X_train, Y_train = creation_useful_dataset(df_train)\n",
    "    print('Importation of the Validation set')\n",
    "    X_validation, Y_validation = creation_useful_dataset(df_validation)\n",
    "    print('Importation of the Testing set')\n",
    "    X_test, Y_test = creation_useful_dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78a2cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The convolutional network (U-net)\n",
    "class UNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Contracting Path\n",
    "        self.conv11 = nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv12 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv21 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv22 = nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv31 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv32 = nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.conv41 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv42 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.conv51 = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv52 = nn.Conv2d(in_channels=1024,out_channels=1024,kernel_size=3, stride=1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Expanding Path\n",
    "        self.up5 = nn.Upsample(size=(62,62))\n",
    "        self.conv53 = nn.Conv2d(in_channels = 1024, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.conv61 = nn.Conv2d(in_channels=1024,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv62 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.up6 = nn.Upsample(size=(125,125))\n",
    "        self.conv63 = nn.Conv2d(in_channels = 512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.conv71 = nn.Conv2d(in_channels=512,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv72 = nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.up7 = nn.Upsample(size=(250,250))\n",
    "        self.conv73 = nn.Conv2d(in_channels = 256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.conv81 = nn.Conv2d(in_channels=256,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv82 = nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.up8 = nn.Upsample(size=(500,500))\n",
    "        self.conv83 = nn.Conv2d(in_channels = 128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.conv91 = nn.Conv2d(in_channels=128,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv92 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv93 = nn.Conv2d(in_channels=64,out_channels=2,kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Output\n",
    "        self.conv10 = nn.Conv2d(in_channels=2,out_channels=1,kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contracting Path\n",
    "        x1 = F.relu(self.conv11(x))\n",
    "        x1 = F.relu(self.conv12(x1))\n",
    "        \n",
    "        x2 = self.pool1(x1)\n",
    "        x2 = F.relu(self.conv21(x2))\n",
    "        x2 = F.relu(self.conv22(x2))\n",
    "        \n",
    "        x3 = self.pool2(x2)\n",
    "        x3 = F.relu(self.conv31(x3))\n",
    "        x3 = F.relu(self.conv32(x3))\n",
    "        \n",
    "        x4 = self.pool3(x3)\n",
    "        x4 = F.relu(self.conv41(x4))\n",
    "        x4 = F.relu(self.conv42(x4))\n",
    "        \n",
    "        x5 = self.pool4(x4)\n",
    "        x5 = F.relu(self.conv51(x5))\n",
    "        x5 = F.relu(self.conv52(x5))\n",
    "\n",
    "        # Expanding Path\n",
    "        x5 = self.up5(x5)\n",
    "        x6 = self.conv53(x5)\n",
    "        x4 = transforms.CenterCrop(62)(x4)\n",
    "        x6 = cat([x4, x6], axis=1)\n",
    "        x6 = F.relu(self.conv61(x6))\n",
    "        x6 = F.relu(self.conv62(x6))\n",
    "        \n",
    "        x6 = self.up6(x6)\n",
    "        x7 = self.conv63(x6)\n",
    "        x3 = transforms.CenterCrop(125)(x3)\n",
    "        x7 = cat([x3, x7], axis=1)\n",
    "        x7 = F.relu(self.conv71(x7))\n",
    "        x7 = F.relu(self.conv72(x7))\n",
    "        \n",
    "        x7 = self.up7(x7)\n",
    "        x8 = self.conv73(x7)\n",
    "        x2 = transforms.CenterCrop(250)(x2)\n",
    "        x8 = cat([x2, x8], axis=1)\n",
    "        x8 = F.relu(self.conv81(x8))\n",
    "        x8 = F.relu(self.conv82(x8))\n",
    "        \n",
    "        x8 = self.up8(x8)\n",
    "        x9 = self.conv83(x8)\n",
    "        x1 = transforms.CenterCrop(500)(x1)\n",
    "        x9 = cat([x1, x9], axis=1)      \n",
    "        x9 = F.relu(self.conv91(x9))\n",
    "        x9 = F.relu(self.conv92(x9))\n",
    "        x9 = F.relu(self.conv93(x9))\n",
    "\n",
    "        # Output\n",
    "        x10 = self.conv10(x9)\n",
    "        x10 = torch.sigmoid(x10)\n",
    "        return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "506ae119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Contracting Path\n",
    "        self.conv11 = nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3, stride=1, padding=1)        \n",
    "        self.bn11 = nn.BatchNorm2d(num_features = 64, eps = 0.00001, momentum = 0.1)\n",
    "        self.conv12 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn12 = nn.BatchNorm2d(num_features = 64, eps = 0.00001, momentum = 0.1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn21 = nn.BatchNorm2d(num_features = 128, eps = 0.00001, momentum = 0.1)\n",
    "        self.conv22 = nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn22 = nn.BatchNorm2d(num_features = 128, eps = 0.00001, momentum = 0.1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv31 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn31 = nn.BatchNorm2d(num_features = 256, eps = 0.00001, momentum = 0.1)\n",
    "        self.conv32 = nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn32 = nn.BatchNorm2d(num_features = 256, eps = 0.00001, momentum = 0.1)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv41 = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn41 = nn.BatchNorm2d(num_features = 512, eps = 0.00001, momentum = 0.1)\n",
    "        self.conv42 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn42 = nn.BatchNorm2d(num_features = 512, eps = 0.00001, momentum = 0.1)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv51 = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn51 = nn.BatchNorm2d(num_features = 1024, eps = 0.00001, momentum = 0.1)\n",
    "        self.conv52 = nn.Conv2d(in_channels=1024,out_channels=1024,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn52 = nn.BatchNorm2d(num_features = 1024, eps = 0.00001, momentum = 0.1)\n",
    "        self.pool5 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Expanding Path\n",
    "        self.up5 = nn.Upsample(size=(62,62))\n",
    "        self.conv53 = nn.Conv2d(in_channels = 1024, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn53 = nn.BatchNorm2d(num_features = 512, eps = 0.00001, momentum = 0.1)\n",
    "        \n",
    "        self.conv61 = nn.Conv2d(in_channels=1024,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn61 = nn.BatchNorm2d(num_features = 512, eps = 0.00001, momentum = 0.1)\n",
    "        self.conv62 = nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn62 = nn.BatchNorm2d(num_features = 512, eps = 0.00001, momentum = 0.1)\n",
    "        self.up6 = nn.Upsample(size=(125,125))\n",
    "        self.conv63 = nn.Conv2d(in_channels = 512, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn63 = nn.BatchNorm2d(num_features = 256, eps = 0.00001, momentum = 0.1)\n",
    "\n",
    "        self.conv71 = nn.Conv2d(in_channels=512,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn71 = nn.BatchNorm2d(num_features = 256, eps = 0.00001, momentum = 0.1)\n",
    "        self.conv72 = nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn72 = nn.BatchNorm2d(num_features = 256, eps = 0.00001, momentum = 0.1)\n",
    "        self.up7 = nn.Upsample(size=(250,250))\n",
    "        self.conv73 = nn.Conv2d(in_channels = 256, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn73 = nn.BatchNorm2d(num_features = 128, eps = 0.00001, momentum = 0.1)\n",
    "        \n",
    "        self.conv81 = nn.Conv2d(in_channels=256,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn81 = nn.BatchNorm2d(num_features = 128, eps = 0.00001, momentum = 0.1)\n",
    "        self.conv82 = nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn82 = nn.BatchNorm2d(num_features = 128, eps = 0.00001, momentum = 0.1)\n",
    "        self.up8 = nn.Upsample(size=(500,500))\n",
    "        self.conv83 = nn.Conv2d(in_channels = 128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn83 = nn.BatchNorm2d(num_features = 64, eps = 0.00001, momentum = 0.1)\n",
    "        \n",
    "        self.conv91 = nn.Conv2d(in_channels=128,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn91 = nn.BatchNorm2d(num_features = 64, eps = 0.00001, momentum = 0.1)\n",
    "        self.conv92 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn92 = nn.BatchNorm2d(num_features = 64, eps = 0.00001, momentum = 0.1)\n",
    "        self.conv93 = nn.Conv2d(in_channels=64,out_channels=2,kernel_size=3, stride=1, padding=1)\n",
    "        self.bn93 = nn.BatchNorm2d(num_features = 2, eps = 0.00001, momentum = 0.1)\n",
    "\n",
    "        # Output\n",
    "        self.conv10 = nn.Conv2d(in_channels=2,out_channels=1,kernel_size=1)\n",
    "        self.bn10 = nn.BatchNorm2d(num_features = 1, eps = 0.00001, momentum = 0.1)\n",
    "        \n",
    "        # The Dropout Rate of the Network\n",
    "        self.dropout = nn.Dropout(0.2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Contracting Path\n",
    "        x1 = F.relu(self.bn11(self.conv11(x)))\n",
    "        x1 = F.relu(self.bn12(self.conv12(x1)))\n",
    "        x1 = self.pool1(x1)\n",
    "        x2 = self.dropout(x1)\n",
    "        \n",
    "        x2 = F.relu(self.bn21(self.conv21(x2)))\n",
    "        x2 = F.relu(self.bn22(self.conv22(x2)))\n",
    "        x2 = self.pool2(x2)\n",
    "        x3 = self.dropout(x2)\n",
    "        \n",
    "        x3 = F.relu(self.bn31(self.conv31(x3)))\n",
    "        x3 = F.relu(self.bn32(self.conv32(x3)))\n",
    "        x3 = self.pool3(x3)\n",
    "        x4 = self.dropout(x3)\n",
    "        \n",
    "        x4 = F.relu(self.bn41(self.conv41(x4)))\n",
    "        x4 = F.relu(self.bn42(self.conv42(x4)))\n",
    "        x4 = self.pool4(x4)\n",
    "        x5 = self.dropout(x4)\n",
    "        \n",
    "        x5 = F.relu(self.bn51(self.conv51(x5)))\n",
    "        x5 = F.relu(self.bn52(self.conv52(x5)))\n",
    "\n",
    "        # Expanding Path\n",
    "        x5 = self.up5(x5)\n",
    "        x6 = self.bn53(self.conv53(x5))\n",
    "        x4 = transforms.CenterCrop(62)(x4)\n",
    "        x6 = cat([x4, x6], axis=1)\n",
    "        x6 = self.dropout(x6)\n",
    "        x6 = F.relu(self.bn61(self.conv61(x6)))\n",
    "        x6 = F.relu(self.bn62(self.conv62(x6)))\n",
    "        \n",
    "        x6 = self.up6(x6)\n",
    "        x7 = self.bn63(self.conv63(x6))\n",
    "        x3 = transforms.CenterCrop(125)(x3)\n",
    "        x7 = cat([x3, x7], axis=1)\n",
    "        x7 = self.dropout(x7)\n",
    "        x7 = F.relu(self.bn71(self.conv71(x7)))\n",
    "        x7 = F.relu(self.bn72(self.conv72(x7)))\n",
    "        \n",
    "        x7 = self.up7(x7)\n",
    "        x8 = self.bn73(self.conv73(x7))\n",
    "        x2 = transforms.CenterCrop(250)(x2)\n",
    "        x8 = cat([x2, x8], axis=1)\n",
    "        x8 = self.dropout(x8)\n",
    "        x8 = F.relu(self.bn81(self.conv81(x8)))\n",
    "        x8 = F.relu(self.bn82(self.conv82(x8)))\n",
    "        \n",
    "        x8 = self.up8(x8)\n",
    "        x9 = self.bn83(self.conv83(x8))\n",
    "        x1 = transforms.CenterCrop(500)(x1)\n",
    "        x9 = cat([x1, x9], axis=1)      \n",
    "        x9 = self.dropout(x9)\n",
    "        x9 = F.relu(self.bn91(self.conv91(x9)))\n",
    "        x9 = F.relu(self.bn92(self.conv92(x9)))\n",
    "        x9 = F.relu(self.bn93(self.conv93(x9)))\n",
    "\n",
    "        # Output\n",
    "        x10 = self.bn10(self.conv10(x9))\n",
    "        x10 = torch.sigmoid(x10)\n",
    "        return x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2aecbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function : Intersection over Union (IoU)\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(IoULoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # Flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        # Intersection is equivalent to True Positive count\n",
    "        # Union is the mutually inclusive area of all labels & predictions \n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection \n",
    "        IoU = (intersection + smooth)/(union + smooth)\n",
    "        return 1 - IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d19fa9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model n°0\n",
      "   Epoch n°1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [01:06<00:00, 33.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model 1 Trained and pickled\n",
      "Model n°1\n",
      "   Epoch n°1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 2/2 [01:29<00:00, 44.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model 2 Trained and pickled\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Settings of the model \n",
    "net1 = UNetV2()      # Simple model\n",
    "net2 = UNetV3()     # Model with dropout and BatchNorm2d\n",
    "criterion = IoULoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-6)\n",
    "models = [net1,net2]\n",
    "\n",
    "# Batch Creation for the training of the model\n",
    "batch_size = 2\n",
    "batches_index_train = list(gen_batches(X_train.shape[0],batch_size=batch_size))\n",
    "n_epoch = 1\n",
    "\n",
    "for j,net in enumerate(models) : \n",
    "    # Someful paramaters \n",
    "    losses_values_per_batches = []\n",
    "    print(f'Model n°{j}')\n",
    "    for epoch in range(n_epoch) : # loop over the dataset multiple times\n",
    "        print(f'   Epoch n°{epoch+1}/{n_epoch}')\n",
    "        for i in tqdm(range(len(batches_index_train))) : \n",
    "            batch_slice = batches_index_train[i]                         # Get the index of the batch\n",
    "            inputs, masks = X_train[batch_slice], Y_train[batch_slice]   # Get inputs and masks\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Step : forward + backward + optimize\n",
    "            outputs = torch.squeeze(net(inputs),dim=1)     # To reduce the dimension to [B,H,W] of outputs\n",
    "            outputs = (outputs>0.5)*1                      # From probabilities to [0,1] for mask and loss computation\n",
    "            # Computation of loss function\n",
    "            loss = criterion(outputs, masks)               # Compute 1-IoT as loss value \n",
    "            loss.requires_grad = True                      # We add a gradient to the loss tensor\n",
    "            loss.backward()                                # Backward of loss function\n",
    "            optimizer.step()                               # We optimize the network\n",
    "\n",
    "            ## We compute loss for this batch on the validation set\n",
    "            net.eval()                                     # We want to evaluate the model\n",
    "            inputs, masks = X_validation, Y_validation     # Get the inputs and masks of validation set \n",
    "            outputs = torch.squeeze(net(inputs),dim=1)>0.5 # Output of the model    \n",
    "            loss = criterion((outputs), masks)             # Compute 1-IoT as loss value \n",
    "            losses_values_per_batches.append(loss.mean())  # Append the mean loss of the batch \n",
    "            net.train()                                    # We turn the model to training mode \n",
    "    \n",
    "    if(j == 0) : # If net1 == U_netV2\n",
    "        # Pickle the model \n",
    "        with open('./pickles/cnn_model', 'wb') as f:\n",
    "             pickle.dump(net, f)\n",
    "\n",
    "        print('   Model 1 Trained and pickled /n')\n",
    "\n",
    "        # We create a plot/png of the loss of the model for epoch batch \n",
    "        plt.plot(range(len(batches_index_train)*n_epoch),losses_values_per_batches)\n",
    "        plt.title('1-IoU loss per epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss (1-IoU)')\n",
    "        plt.ylim(0,1.05)\n",
    "        plt.savefig('./pickles/graph_loss_per_batch.png')\n",
    "    \n",
    "    if(j == 1) : # If net2 == U_netV3\n",
    "        # Pickle the model \n",
    "        with open('./pickles/cnn_model_improved', 'wb') as f:\n",
    "             pickle.dump(net, f)\n",
    "\n",
    "        print('   Model 2 Trained and pickled /n')\n",
    "\n",
    "        # We create a plot/png of the loss of the model for epoch batch \n",
    "        plt.plot(range(len(batches_index_train)*n_epoch),losses_values_per_batches)\n",
    "        plt.title('1-IoU loss per epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss (1-IoU)')\n",
    "        plt.ylim(0,1.05)\n",
    "        plt.savefig('./pickles/graph_loss_per_batch_improved.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ae362",
   "metadata": {},
   "source": [
    "# Importation of the DL network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c998a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model \n",
    "with open('./pickles/cnn_model', 'rb') as f:\n",
    "     net = pickle.load(f)\n",
    "with open('./pickles/cnn_model_improved', 'rb') as f:\n",
    "     net_improved = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ef40f7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is : 0.8936958312988281\n"
     ]
    }
   ],
   "source": [
    "# Testing the model \n",
    "outputs = torch.squeeze(net(X_test),dim=1)\n",
    "outputs = (outputs>0.5)\n",
    "print(f'The loss is : {criterion(outputs, Y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "668cff82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAU7klEQVR4nO3de3hU9Z3H8fc3kxvhZoICKaGAClboCpaI11oqtbDoI7q7trjV0paW1equ1HZdWLtuqdraar2V2i1VKz5ekKpPoWqLSPWxtVYMCFbuURAiqUhABDEhl+/+kdN2gEwSyJw5M4fP63nmmZlzzpzfd8I3H05+Zy7m7oiISLzkRV2AiIikn8JdRCSGFO4iIjGkcBcRiSGFu4hIDCncRURiKLRwN7MJZrbOzKrNbEZY44hkkvpacoWF8Tp3M0sA64FzgRrgFeASd1+d9sFEMkR9LbkkrCP3MUC1u7/p7vuAecCkkMYSyRT1teSMsMJ9ALAl6X5NsEwkl6mvJWfkh7Rfa2PZfvM/ZjYNmAaQIDG6hF4hlSICu9m53d2P6eJuOuxr2L+3u5fY6I8dX9jFYUXatmlLI9t3NLfVl6GFew0wMOl+BbA1eQN3nwPMAehlZX6qjQupFBF41h97Kw276bCvYf/erhxZ7EsXDTxwE5G0GDN+S8p1YU3LvAIMNbMhZlYITAYWhjSWSKaoryVnhHLk7u5NZnYVsAhIAPe5+6owxhLJFPW15JKwpmVw96eBp8Pav0gU1NeSK/QOVRGRGFK4i4jEkMJdRCSGFO4iIjGkcBcRiSGFu4hIDCncRURiSOEuIhJDCncRkRhSuIuIxJDCXUQkhhTuIiIxpHAXEYkhhbuISAwp3EVEYkjhLiISQwp3EZEYUriLiMSQwl1EJIYU7iIiMaRwFxGJIYW7iEgMdRjuZnafmW0zs9eTlpWZ2WIz2xBclyatm2lm1Wa2zszGh1W4SFeptyXOOnPkfj8w4YBlM4Al7j4UWBLcx8yGA5OBEcFj7jazRNqqFUmv+1FvS0x1GO7u/gKw44DFk4C5we25wIVJy+e5e4O7bwSqgTHpKVUkvdTbEmeHO+fez91rAYLrvsHyAcCWpO1qgmUHMbNpZlZlZlWNNBxmGSJpl9befreuOdRiRVJJ9wlVa2OZt7Whu89x90p3ryygKM1liKTdYfX2MX00cyPRONxwf8fMygGC623B8hpgYNJ2FcDWwy9PJOPU2xILhxvuC4Epwe0pwIKk5ZPNrMjMhgBDgaVdK1Eko9TbEgv5HW1gZo8AY4GjzawG+F/gZmC+mU0FNgMXA7j7KjObD6wGmoAr3V2TjpKV1NsSZx2Gu7tfkmLVuBTb3wTc1JWiRDJBvS1xpneoiojEkMJdRCSGFO4iIjGkcBcRiSGFu4hIDCncRURiSOEuIhJDCvcckzi6D1hbH3MiIvJ3CvcckRh6LG/cehqffn4TTed8IupyRCTLdfgOVYlWok8ZtZ//GF+96tf8uvdbJCyPlTcMZMeKMprrDvwochGRVjpyz1JWUEjDxFMY+JsPefG/7+DKo7aQsNZ/rl8MWkL1t07Q9IyIpKRwz0JN40az41eDePD/budnFS9Rkle43/oCS/DYJbfT9GlNz4hI2zQtk0USI05g7dePYt7E2YwpKgB6pNz2pMJi+t3wJu+91ofm7XWZK1JEcoKO3LNAol9far95Bl984hnevOhnQbB3bO7gZ6m+ZpimZ0TkIAr3CFlREdunnc4Zizez7JofM7nnzkN6fIEleOSSO2kcp+kZkSNRbVNxynWalomCGfXnnULhN2pZNOxWjk50Bw7vuzZHFxXSf9abvLdC0zMiR5q9zYUp1+nIPcNs9AjW3zuau2ffyeITfx0Ee9f8YvAzVH9rWBqqE5FcMqRoV8p1CvcMSfTry5Zvn8H0Rx9j44R7GFHYLW37LrICfnrxHPJGDU/bPkUktyncQ5ZXUsKuL5zG0b+q589XzGZCSUMo44zr1szO7+0jr3vX/xIQkdywsaF3ynUK97CY4WeOYtujFSz6we08MOiFv70JKSy/O+lhtk4bGeoYIpI9mjx1pijcQ5A/ZBAbv3caP3zwZywbPZ/eeembgmlPSV4hN3z9fmz0iIyMJyLRKshrTrmuw3A3s4Fm9pyZrTGzVWZ2dbC8zMwWm9mG4Lo06TEzzazazNaZ2fi0PIsckOjVi3cvP51//e0fWPPFnzCqqCjjNVzQfS91sxo1PdMJ6m3Jdd3yGlOu68yRexPwTXc/ETgNuNLMhgMzgCXuPhRYEtwnWDcZGAFMAO42s8N7nV+OsPx8msaNJm9hd/7w7Tv5Qs+60Kdg2vPcqAd4+980PdMJ6m3JaeX59SnXdZhA7l7r7suD27uBNcAAYBIwN9hsLnBhcHsSMM/dG9x9I1ANjDnc4rNdYthxbHxoOPfcdydPDvvNQZ8DE4UeecV894oHsJM1PdMe9bbE2SEdXprZYOBk4GWgn7vXQusvCdA32GwAsCXpYTXBstjZdelpXP7Ub1h91v0MKUj9OTBRuLD7HnbcuI+8kpKoS8kJ6m2Jm06Hu5n1AB4Hprv7++1t2sYyb2N/08ysysyqGgnn5YFh6761kY/m74x0CqY9i0c+wNuXj4q6jKwXZm+/W5f6hJdImDqVSmZWQGvzP+TuTwSL3zGz8mB9ObAtWF4DDEx6eAWw9cB9uvscd69098oCMn/iMR3yn1vO5x6dToOnPqkRpd553fjK1KfJHzIo6lKyVti9fUwfTclLNDrzahkD7gXWuPttSasWAlOC21OABUnLJ5tZkZkNAYYCS9NXchZx5/gfrGXqW+dGXUlK00s3seY7fbCC6M8FZBv1tsRZZ47czwQuA84xsxXBZSJwM3CumW0Azg3u4+6rgPnAauC3wJXuHtu/TZt37mTr9cexrGFf1KWk9OKn72Ln5NFRl5GN1NsSW+Z+0JRhxvWyMj/VxkVdRpdsvPl0Vl02m4IsfWXcLTuO47kLT6K5emPUpUTiWX9smbtXZnrcypHFvnTRwI43FDkMY8ZvoWplfZtf6JCdZwJz0PG3rmfKps9EXUZK/1n2BmuvL9X0jMgRQuGeJs3b6/jLrONY0ZC9r/x5Yexd7LxE0zMicdHQzqygwj2NCp59lc/Nm06zt0RdSpsq8nvw+WsXkX/s4KhLEZE02LyvLOU6hXs6tTRz/I/W8+XNY6OuJKWrS6tZO6sU8rLz3ICIdN7gotRfzalwT7Pm7XVU3zmczU17oi6lTQnL4w+f+jF/ufpUPpw0Rl+uLZLDCtqJcIV7CHrOf4VPLc7e6Zny/B688M0f8V8/egCr/HjU5YhICBTuYWhpZvgN2/h+XfZ+7V3vvG6cV1LP7hs+IK9nz6jLEZE0U7iHpGnTZp76/tisnZ75q0Uff5iaK/4h6jJEJM0U7iHq/csqPvXM9KjLaFePvGJunnYfdooCXiROFO4h8qYmht+4jRu3fyzqUtp1Xkk9e2/co29vEokRhXvImjZt5unvjaU2y6dnnh7xCFuu0rc3icSFwj0Dej2+nDOeyd5Xz0Dr9MxtX/u5Xj0jEhMK9wzwxn0Mn/UXbts5NOpS2vXZkkb23vSBvr1JJAYU7hnStKWGh+aMZ2fz3qhLaddvRzzK21eMiroMEekihXsGlc9ZzjnLvxJ1Ge0qySvka195isSw46IuRUS6QOGeQS319fS/3pi3uzTqUtr176Vvsf47PfXxwCI5TOGeYS0r13Dz7EvY1fJh1KW06/dnzabuMn08sEiuUrhHoPy+lZxdld3TM+X5PfjSt54kMfTYqEsRkcOgcI9Aywcf8JHrncf39Iq6lHZd3vstNszqheXnR12KiBwihXtEWl5by413XcqelvqoS0kpYXm8+MmfUHfZKVGXIiKHSOEeof73LOeTy74UdRnt6pvozteuXUDi+CFRlyIih0DhHqGW+nr6XwfP7C2IupR2Teu9lfXf7Y0VFUVdioh0UofhbmbFZrbUzFaa2SozmxUsLzOzxWa2IbguTXrMTDOrNrN1ZjY+zCeQ61peX8t/PPxVGrwx6lLa9con72bXP50cdRlppd6WOOvMkXsDcI67jwRGARPM7DRgBrDE3YcCS4L7mNlwYDIwApgA3G1m+sLOdhx75zo+X31+1GW0qzRRwuhrXiW/f7+oS0kn9bbEVofh7q3++pGGBcHFgUnA3GD5XODC4PYkYJ67N7j7RqAaGJPOouOmuW4HO24flPVf7HH7R/7Ipi/H552r6m2Js07NuZtZwsxWANuAxe7+MtDP3WsBguu+weYDgC1JD68Jlh24z2lmVmVmVY00dOEpxEO3hcuy/os9CizB5Zc+RX55/6hLSZuwe/vduuZQ6xdJpVPh7u7N7j4KqADGmFl7nwtrbe2ijX3OcfdKd68sQCfqaGnmxNve5/73+3a8bYSGF9dQf+JBeZazwu7tY/po1kaicUivlnH394DnaZ1vfMfMygGC623BZjXAwKSHVQBbu1rokaB59XpumfsvWXlytaZpDye+eBm3XnQx+c8tj7qctFNvS9x05tUyx5jZUcHtbsBngLXAQmBKsNkUYEFweyEw2cyKzGwIMBRYmua6Y+ujs//MResnRV3G3zR4IxdsmMClV3yDQZeup+W1teAHHazmJPW2xFln3ldeDswNXhWQB8x39yfN7CVgvplNBTYDFwO4+yozmw+sBpqAK91dE4+d1LJ7N37dsbz2cD0nFRZHWsusd4fzq5+PpfyhNRTtfOXg+Yfcp96W2DLPgqOwXlbmp9q4qMvIHnkJNtxRyYZ//ikJy/z7zJ7aW8zVC77ECXe9TdNbWzp+QA541h9b5u6VmR63cmSxL100sOMNRQ7DmPFbqFpZ39a5oE4duUumtTRz4m213HtuBdN6Z25K943GPVy4fBoVs+C4FX+iKWMji0i66eMHslTTps38/JZJGflgsT0t9UzdfBZfnTadARevp2XF6tDHFJFwKdyz2NG/fJ2Jqy4Jbf/N3sKcXR9h7He+Qe1n8yhcVIU36XhdJA4U7lmsZfduuv1PD6bXVnLvrv6s2beXvS37aEzDObwX6mH4/VeyYOIp9LnnJZrffz8NFYtIttCce7Zb+mfWnd2d9Ylh/HLYZ2joU8zOYYXsPq6FRP+9XHTCaxRYM18s/RNH5UFpXjEF7XzcyRuNe5jw4lUMme0M/uNLmlcXiSmFew5o+eCD1htVr1MI9FsE/QDMWJlfiCXyWDZsKl5UwDun9qShNzSftIcT+7/D2X02ML7HKkqsmZk1F1Bz+1COfaIKWvQKPpE4U7jnMne8cR/eCLy2FoC+r/x99YdmPFs2mGdLR9JS2oO8DZvp/t7L0dQqIhmlcI8zd5rrdkDdDgB0rC5y5NAJVRGRGFK4i4jEkMJdRCSGFO4iIjGkcBcRiSGFu4hIDCncRURiSOEuIhJDCncRkRhSuIuIxJDCXUQkhhTuIiIxpHAXEYmhToe7mSXM7FUzezK4X2Zmi81sQ3BdmrTtTDOrNrN1ZjY+jMJF0kF9LbmskZaU6w7lyP1qYE3S/RnAEncfCiwJ7mNmw4HJwAhgAnC3WTtfDSQSLfW15KxN9aUp13Uq3M2sAjgPuCdp8SRgbnB7LnBh0vJ57t7g7huBamDMoZUsEj71teS6Fizlus4eud8BXAv7/Q3Qz91rAYLrvsHyAcCWpO1qgmUi2eYO1NcSUx2Gu5mdD2xz92Wd3Gdb/5V4G/udZmZVZlbVSEMndy2SHmH1dbDvv/X2u3X6/isJT76lnnPvzNfsnQlcYGYTgWKgl5k9CLxjZuXuXmtm5cC2YPsaYGDS4yuArQfu1N3nAHMAellZm78kIiEKpa9h/96uHFms3pbQDCnalXJdh0fu7j7T3SvcfTCtJ5R+5+6XAguBKcFmU4AFwe2FwGQzKzKzIcBQYOnhly+SfupriYO8dubcu/IF2TcD881sKrAZuBjA3VeZ2XxgNdAEXOnu+ttUcoX6WmLhkMLd3Z8Hng9u1wHjUmx3E3BTF2sTyQj1tcSR3qEqIhJDCncRkRhSuIuIxJDCXUQkhhTuIiIxpHAXEYkhhbuISAwp3EVEclRDO++jU7iLiOSoD7ww5TqFu4hIjirL05G7iMgRReEuIhJDCncRkRhSuIuIxJDCXUQkhhTuIiIxpHAXEYkhhbuISAwp3EVEYkjhLiISQwp3EZEYUriLiMSQwl1EJIYU7iIiMaRwFxGJIXP3qGvAzHYD6yIa/mhg+xE0bpRjR/mcB7n7MZkeVL19RIwb5dgp+zo/05WksM7dK6MY2Myqohg7qnGjHDvK5xwh9XbMx4167FQ0LSMiEkMKdxGRGMqWcJ9zBI6t53xk0L9z/MeNeuw2ZcUJVRERSa9sOXIXEZE0ijzczWyCma0zs2ozm5Hmfd9nZtvM7PWkZWVmttjMNgTXpUnrZgZ1rDOz8V0ce6CZPWdma8xslZldnYnxzazYzJaa2cpg3FkZft4JM3vVzJ7M5LjZKI69HVVfB/tRbx8Kd4/sAiSAN4BjgUJgJTA8jfs/G/gE8HrSsh8CM4LbM4AfBLeHB+MXAUOCuhJdGLsc+ERwuyewPhgj1PEBA3oEtwuAl4HTMvi8rwEeBp7M5M872y5x7e2o+lq9fRj1RtH4ST+s04FFSfdnAjPTPMbgA34B1gHlSY26rq2xgUXA6WmsYwFwbibHB0qA5cCpmRgXqACWAOck/QJE8vOO+nKk9HYUfR3sR73dwSXqaZkBwJak+zXBsjD1c/dagOC6b9i1mNlg4GRajzRCHz/483EFsA1Y7O4ZGRe4A7gWaElalvGfd5aIfW9nuq+DMdXbnRR1uFsby6J6+U4otZhZD+BxYLq7v5+J8d292d1H0Xq0McbMPh72uGZ2PrDN3Zd19iHpGDeLZdPzS3stUfQ1qLcPRdThXgMMTLpfAWwNecx3zKwcILjeFlYtZlZA6y/AQ+7+RKbHd/f3gOeBCRkY90zgAjPbBMwDzjGzBzMwbraKbW9H3deg3u6UTM8DHTCPlQ+8SetJh7+edBqR5jEGs/+85C3sfxLkh8HtEex/EuRNunbyxYAHgDsOWB7q+MAxwFHB7W7A74HzM/W8g32O5e/zkhkbN5suce3tqPpavX0YtWZ6wDZ+WBNpPeP+BnBdmvf9CFALNNL6v+lUoA+tJ0Y2BNdlSdtfF9SxDvjHLo59Fq1/ir0GrAguE8MeHzgJeDUY93Xg+mB5Rp53sL/kX4CMjZttlzj2dlR9rd4+9IveoSoiEkNRz7mLiEgIFO4iIjGkcBcRiSGFu4hIDCncRURiSOEuIhJDCncRkRhSuIuIxND/A0Bv1yFbaIPfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting for rigolade \n",
    "for i, prediction in enumerate(outputs[:3]) :\n",
    "    fig,ax = plt.subplots(1,2)\n",
    "    true_mask = np.array(Y_test[i])\n",
    "    prediction = np.array(prediction)\n",
    "    ax[0].imshow(true_mask)\n",
    "    ax[1].imshow(prediction)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1596685",
   "metadata": {},
   "source": [
    "# To do list :\n",
    "\n",
    "**Optimizer :** We can try with different optimizers to see some differences.\n",
    "\n",
    "**Batch :** Export and Import batch data for each epoch to use less memory.\n",
    "\n",
    "**Metrics :** We can use intersection over union (IoU) with the masks, AUC, position recall curve.\n",
    "\n",
    "**Expansion of the model :** We can test the model on unseen data to see if it can be generalized.\n",
    "\n",
    "**Create a validation set :** In order to validate of model for each epoch, we can compute the loss withthe validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
